{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalinc/Introduction_to_LLMs/blob/main/Amalin_Cohen_Assignment_01_Introduction__to__LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Replace 'your_key_here' with your actual API key\n",
        "# env_content = \"\"\"\n",
        "# OPENAI_API_KEY=key\n",
        "# GEMINI_API_KEY=key\n",
        "# \"\"\"\n",
        "\n",
        "# with open(\".env\", \"w\") as f:\n",
        "#     f.write(env_content.strip())\n",
        "\n",
        "# print(\".env file created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rqIqqbA9s1v",
        "outputId": "a75df10f-3ee0-419a-b78c-e0cd4d4b04c1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".env file created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new_key = \"\"\n",
        "# g_new_key = \"\"\n",
        "\n",
        "# with open(\".env\", \"w\") as f:\n",
        "#     f.write(f\"OPENAI_API_KEY={new_key}\\n\")\n",
        "#     f.write(f\"GEMINI_API_KEY={g_new_key}\\n\")\n",
        "\n",
        "# print(\".env file overwritten with new key.\")\n"
      ],
      "metadata": {
        "id": "81vPpFnHGdiL",
        "outputId": "c1e065c3-be48-4437-83e4-26f9a7642bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".env file overwritten with new key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env"
      ],
      "metadata": {
        "id": "5KY98HrYM6AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24MQeFuQjY-t"
      },
      "source": [
        "# Introduction to Large Language Models and Prompt Engineering\n",
        "\n",
        "**Course:** GenAI development and LLM applications\n",
        "\n",
        "\n",
        "**Instructors:** Ori Shapira, Yuval Belfer\n",
        "\n",
        "**Semester:** Summer\n",
        "    \n",
        "## Overview\n",
        "\n",
        "This assignment provides a **hands‑on tour of the GenAI workflow**: from calling different kinds of language models to critically comparing their outputs and evaluating real‑world products built on top of them. You will practice with both *open‑source checkpoints* (via Hugging Face) and *hosted APIs* from OpenAI and Google, and reflect on where each approach shines.\n",
        "\n",
        "Along the way you will explore how model size, instruction‑tuning, and deployment UX influence quality, cost, and user trust. The notebook culminates in a creative exercise where you invent a GenAI product to fill an unmet need—connecting technical capability to business value.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Load and run** open‑source, OpenAI, and Google Gemini models using their latest SDKs/APIs.\n",
        "- **Compare** small (≤ 3B) and large (≥ 7 B) models on simple vs. complex prompts.\n",
        "- **Assess** output quality across dimensions such as correctness, coherence, and creativity.\n",
        "- **Evaluate** end‑user GenAI products, identifying UX trade‑offs and guardrails.\n",
        "- **Design** a new GenAI product concept aligned with a specific business pain‑point.\n",
        "\n",
        "## Prerequisites\n",
        "- Basic Python knowledge\n",
        "- Familiarity with Jupyter notebooks\n",
        "- Internet connection for API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5FE7dXVjY-v"
      },
      "source": [
        "## \\*\\*IMPORTANT NOTE\n",
        "\n",
        "* **DO NOT HESITATE TO USE Gen AI tools such as ChatGPT to HELP you solve this**\n",
        "\n",
        "* **DO NOT USE Gen AI tools such as ChatGPT to solve this for you**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSDdE5yWjY-v"
      },
      "source": [
        "# 1. Main stream APIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKOD6W1CjY-v"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ist6EdU-jY-w"
      },
      "source": [
        "### Setup Guide: OpenAI & Google Gemini APIs\n",
        "\n",
        "Follow these steps to get your notebook ready to call the mainstream LLM APIs used in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qXX_uoJjY-w"
      },
      "source": [
        "#### 1. Install Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q_BoUBCjY-w"
      },
      "outputs": [],
      "source": [
        "# Install necessary Python packages\n",
        "%pip install --upgrade openai google-generativeai python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUTZUKGDjY-x"
      },
      "source": [
        "If you prefer installing from a terminal instead of within Colab/Jupyter, the equivalent command is:\n",
        "\n",
        "```bash\n",
        "pip install --upgrade openai google-generativeai python-dotenv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek8O5E5YjY-x"
      },
      "source": [
        "#### 2. Obtain your API keys\n",
        "\n",
        "| Provider | Where to generate the key |\n",
        "|----------|--------------------------|\n",
        "| **OpenAI** | 1. Sign in at <https://platform.openai.com/>  <br>2. Go to **API Keys → Create new secret key**  <br>3. Copy the key (`sk-…`) |\n",
        "| **Google Gemini** | 1. Visit <https://aistudio.google.com/app/apikey>  <br>2. Click **Create API key** and follow the prompts  <br>3. Copy the key (`AI-…`) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX7eUseVjY-x"
      },
      "source": [
        "#### 3. Store keys as environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl-EuocOjY-x"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Replace the placeholders below with your real keys.\n",
        "# os.environ['OPENAI_API_KEY'] = \"sk-...\"\n",
        "# os.environ['GEMINI_API_KEY'] = \"AI-...\"\n",
        "\n",
        "# print(\"Environment variables set for this notebook session ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olBnJvvbjY-x"
      },
      "source": [
        "Alternatively, you can create a `.env` file in your project directory with the following contents:\n",
        "\n",
        "```text\n",
        "OPENAI_API_KEY=sk-...\n",
        "GEMINI_API_KEY=AI-...\n",
        "```\n",
        "\n",
        "Then load the variables automatically in Python using **python-dotenv**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BX5infNqjY-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da3b875-16fe-4fa0-d3c4-6e272311b8b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # Loads variables from .env into the current environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MtgbOt3jY-y"
      },
      "source": [
        "#### 4. Quick verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6giGX-mnjY-y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "# from google.colab import userdata\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "# genai.configure(api_key= userdata.get('GEMINI_API_KEY') )\n",
        "\n",
        "# Try listing available OpenAI models (show first 3)\n",
        "print(\"OpenAI models:\", [m.id for m in openai.models.list().data[:3]])\n",
        "\n",
        "# List available Gemini models that support generateContent\n",
        "print(\"\\nAvailable Gemini models that support generateContent:\")\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env\n"
      ],
      "metadata": {
        "id": "WJ2Si6rWIJ21",
        "outputId": "0231a074-0972-4437-fb4b-31edcfb4dbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7jetwzTjY-y"
      },
      "source": [
        "## Using OAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TDGubA1CjY-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85cbf25c-f5f1-4598-f868-5f13987c4e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue primarily due to a phenomenon known as Rayleigh scattering. This involves the scattering of sunlight by the molecules and small particles in Earth's atmosphere.\n",
            "\n",
            "Here's a more detailed explanation:\n",
            "\n",
            "1. **Composition of Sunlight**: Sunlight seems white, but it is actually made up of many different colors, corresponding to different wavelengths. These colors can be seen in a rainbow or when light is passed through a prism.\n",
            "\n",
            "2. **Scattering of Light**: As sunlight travels through the Earth's atmosphere, it collides with gases and particles. Light is scattered in all directions by these molecules and particles. However, different colors of light are scattered by different amounts. \n",
            "\n",
            "3. **Blue Light Scattering**: Blue light (shorter wavelength) is scattered more than red light (longer wavelength) because it travels in smaller and shorter waves. This is why, during the day, we mainly see blue light coming from all directions, making the sky appear blue to us.\n",
            "\n",
            "4. **Variation During the Day**: At sunrise and sunset, the sky can appear red or orange because the light has to travel through a greater thickness of atmosphere. This means more blue and shorter wavelength lights are scattered out of the direct path of the light that reaches our eyes, allowing the longer wavelengths like red and orange to dominate.\n",
            "\n",
            "So, the sky appears blue due to Rayleigh scattering, where shorter wavelengths of light (like blue) are scattered much more than the longer wavelengths (like red).\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Implement `call_openai`\n",
        "# Your task is to complete the function below so that it sends a prompt to the OpenAI\n",
        "# Chat Completion API and returns the assistant's reply as a **string**.\n",
        "#\n",
        "# Requirements ✍️\n",
        "# ------------\n",
        "# 1. Read your key from the environment variable `OPENAI_API_KEY` (do **not** hard‑code keys).\n",
        "# 2. Use `openai.ChatCompletion.create` with the given *model* (default: `gpt-4o-mini`).\n",
        "# 3. The function should take a single user *prompt* and return the model's reply **text only**.\n",
        "# 4. Raise a clear `EnvironmentError` if the key is missing.\n",
        "# 5. Let any `openai.error.OpenAIError` exceptions propagate––don’t swallow them.\n",
        "#\n",
        "# Example\n",
        "# -------\n",
        "# >>> call_openai(\"Explain why the sky is blue.\")\n",
        "# 'Rayleigh scattering causes shorter (blue) wavelengths ...'\n",
        "#\n",
        "# This exercise tests your ability to work with third‑party GenAI REST APIs in Python.\n",
        "#\n",
        "# 🚨 Delete the `raise NotImplementedError` line once you add your code.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def call_openai(prompt: str, model: str = \"gpt-4o\") -> str:\n",
        "    \"\"\"Send *prompt* to the OpenAI Chat Completion API and return the reply text.\"\"\"\n",
        "\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\"Missing OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Let OpenAIError propagate naturally if it occurs\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"Explain why the sky is blue.\"\n",
        "print(call_openai(prompt))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env\n"
      ],
      "metadata": {
        "id": "yu164vNEIz5l",
        "outputId": "f84c5cc3-c74c-4d08-a1de-bf9093f99e38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIagCFtvI3n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gDUShHnjY-y"
      },
      "source": [
        "## Using Google Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwuSXyycjY-y"
      },
      "outputs": [],
      "source": [
        "# Exercise: Implement `call_gemini`\n",
        "# Your task is to complete the function below so that it sends a prompt to Google’s **Gemini** model\n",
        "# (via the `google-generativeai` Python SDK) and returns the model's reply as a **string**.\n",
        "#\n",
        "# Requirements ✍️\n",
        "# ------------\n",
        "# 1. Read your key from the environment variable `GEMINI_API_KEY`.\n",
        "# 2. Use `google.generativeai.GenerativeModel(model).generate_content` to get a response.\n",
        "# 3. Accept the same arguments as `call_openai` (prompt, model) and return *text only*.\n",
        "# 4. Raise `EnvironmentError` if the key is missing.\n",
        "#\n",
        "# Example\n",
        "# -------\n",
        "# >>> call_gemini(\"Suggest three sci‑fi book titles set on Mars.\")\n",
        "# '1. Red Horizon ...'\n",
        "#\n",
        "# 🚨 Delete the `raise NotImplementedError` once you add your code.\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "def call_gemini(prompt: str, model: str = \"gemini-pro\") -> str:\n",
        "    \"\"\"Send *prompt* to the Google Gemini API and return the reply text.\"\"\"\n",
        "    # TODO: Implement this function following the requirements above.\n",
        "    raise NotImplementedError(\"call_gemini() is not implemented yet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240ehahpjY-y"
      },
      "source": [
        "# Open source (Hugging Face)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aunvloFmjY-y"
      },
      "source": [
        "\n",
        "\n",
        "1. Use **Gemma 3 4B** model.\n",
        "2. Ensure your Hugging Face access token (`HF_TOKEN`) is set correctly in your environment (e.g., `export HF_TOKEN=YOUR_TOKEN`), or use `huggingface-cli login`.\n",
        "3. Confirm that the student has **accepted the model terms** for Gemma on Hugging Face before attempting to download or load the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSf40_sqjY-y"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In this subsection we'll install and configure the dependencies needed to load an **open‑source large language model (LLM)** from **Hugging Face**.\n",
        "\n",
        "1. We install the latest versions of the `transformers`, `accelerate`, and `bitsandbytes` libraries.\n",
        "2. If the model you want to use is behind a gated license (e.g. *Llama 2*), create a free Hugging Face account, accept the model license, and export your **HF token** as an environment variable so that `transformers` can download it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUfhUGKQjY-z"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries (run once)\n",
        "!pip -q install --upgrade \"transformers[torch]\" accelerate bitsandbytes --progress-bar off\n",
        "\n",
        "# Set your Hugging Face access token\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPWm1IMTjY-z"
      },
      "source": [
        "## Running Gemma‑3‑4B via `transformers` pipeline\n",
        "\n",
        "Below we load the **Gemma‑3‑4B** instruction‑tuned checkpoint (`google/gemma-3-4b-it`) and generate a short response.  \n",
        "Make sure you have accepted the model license on Hugging Face **and** that your `HF_TOKEN` is set (or run `huggingface-cli login`) before running the cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppROWgqPjY-z"
      },
      "source": [
        "**IMPORTANT** If you are using google colab change your notebook's runtime type to include a GPU. Go to \"Runtime\" -> \"Change runtime type\" and select \"T4 GPU\" under \"Hardware accelerator\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-quWJ-WjY-z"
      },
      "source": [
        "**NOTE:** you can use ***FlashAttention2*** for faster infference. you'll have to install `!pip install flash_attn --no-build-isolation`, update `attn_implementation` and hope that's it :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH-KWjQxjY-z"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    Gemma3ForConditionalGeneration,\n",
        "    pipeline,\n",
        ")\n",
        "import torch\n",
        "\n",
        "# Instruction‑tuned Gemma 3 checkpoint\n",
        "model_id = \"google/gemma-3-4b-it\"\n",
        "\n",
        "# Check for CUDA availability and set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Determine torch_dtype based on device and availability\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16 if torch.cuda.is_available() else \"auto\"\n",
        "print(f\"Using torch_dtype: {torch_dtype}\")\n",
        "\n",
        "\n",
        "# Load tokenizer & model (first run ~8‑10 GB download)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"eager\",             # Fallback to eager attention / You can try \"flash_attention_2\" if you have a GPU with flash attention support\n",
        "    device_map=device,                       # Explicitly set device\n",
        "    torch_dtype=torch_dtype,                 # use bf16/fp16 if available\n",
        "    token=True                               # pick up HF_TOKEN env var\n",
        ")\n",
        "\n",
        "# Build a text‑generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "prompt = \"Explain the difference between supervised and unsupervised learning in two sentences.\"\n",
        "response = generator(prompt)[0][\"generated_text\"]\n",
        "print(\"\\n--- Generated Response ---\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z4X5ZEcjY-z"
      },
      "source": [
        "## Comparing Output Quality: Gemma 3 4B vs. Gemini / GPT‑4o\n",
        "\n",
        "In this section you will **run the *same prompt*** through three different models and compare their outputs:\n",
        "\n",
        "1. **Gemma 3 4B** – use the `generator` pipeline you built earlier (`google/gemma-3-4b-it` checkpoint).  \n",
        "2. **Gemini 1.5 Pro (call_gemini)** – use your `call_gemini(prompt)` helper.  \n",
        "3. **GPT‑4o (call_openai)** – use your `call_openai(prompt)` helper.\n",
        "\n",
        "### What to do\n",
        "* Pick a single prompt of your choice (keep it identical for all three calls).  \n",
        "* Capture each model’s response.  \n",
        "* In a short paragraph, compare the outputs along these axes: **factual accuracy / hallucinations, completeness, style / tone, and overall usefulness**.  \n",
        "* Optionally repeat with a *second* prompt that stresses reasoning or creativity to see whether conclusions change.\n",
        "\n",
        "**Fill in the code cell below and then write your reflection in the provided markdown cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQD92dFHjY-z"
      },
      "outputs": [],
      "source": [
        "# ▶️ Compare outputs from Gemma 3‑4B, Gemini, and GPT‑4o\n",
        "prompt = \"Explain the most important differences between classical mechanics and quantum mechanics in two paragraphs.\"\n",
        "\n",
        "# Gemma (pipeline defined earlier)\n",
        "gemma_out = generator(prompt)[0][\"generated_text\"]\n",
        "\n",
        "# Gemini (helper function you implemented)\n",
        "gemini_out = call_gemini(prompt)\n",
        "\n",
        "# GPT‑4o (helper function you implemented)\n",
        "gpt4o_out = call_openai(prompt)\n",
        "\n",
        "print(\"\\n--- Gemma 3‑4B ---\\n\", gemma_out)\n",
        "print(\"\\n--- Gemini ---\\n\", gemini_out)\n",
        "print(\"\\n--- GPT‑4o ---\\n\", gpt4o_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwwUMkbfjY-z"
      },
      "source": [
        "# Try out a GenAI‑Based Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKwT6QqEjY-0"
      },
      "source": [
        "\n",
        "\n",
        "Choose **one** generative‑AI product (for example:  \n",
        "* **Cursor** – an AI‑powered code editor,  \n",
        "* **GitHub Copilot** – an AI pair‑programmer,  \n",
        "* **Perplexity AI** – an answer‑engine / research assistant,  \n",
        "* **Claude**, **Notion AI**, **Llama Index Lobe**,\n",
        "or any other tool that interests you).  \n",
        "\n",
        "Spend **10–15 minutes** exploring its features. Then:\n",
        "\n",
        "1. Briefly describe the *task or workflow* you tried.  \n",
        "2. List **two things you liked** and **two things you wish were better**.  \n",
        "3. Reflect on how underlying model capabilities (e.g., context window, fine‑tuning, retrieval, chain‑of‑thought) manifest in the product experience.  \n",
        "4. (Optional) Include a screenshot or snippet illustrating something interesting you observed.\n",
        "\n",
        "Use the markdown cell below to record your reflections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj43JWzOjY-0"
      },
      "source": [
        "# What GenAI products does not exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eXTwys6jY-0"
      },
      "source": [
        "## Thinking Exercise – Design a New GenAI Product\n",
        "\n",
        "Invent **one GenAI‑powered product that does not yet exist** but could solve a real problem. Write **100‑150 words** addressing the prompts below.\n",
        "\n",
        "**Prompts**\n",
        "1. **Product name & one‑line pitch**  \n",
        "2. **Target users / market segment**  \n",
        "3. **Pain point / business need** – why current solutions are insufficient  \n",
        "4. **User flow & where GenAI fits** – describe the key interactions  \n",
        "5. **Technical or ethical challenges** – data, safety, IP, bias, etc.  \n",
        "6. **MVP success metric** – what would you measure to confirm traction  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRyDfYtkjY-0"
      },
      "source": [
        "### ✍️ Your concept\n",
        "\n",
        "**1. Product name & pitch**:  \n",
        "\n",
        "**2. Target users**:  \n",
        "\n",
        "**3. Pain point**:  \n",
        "\n",
        "**4. User flow**:  \n",
        "\n",
        "**5. Challenges**:  \n",
        "\n",
        "**6. Success metric**:  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}