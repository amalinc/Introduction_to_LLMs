{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalinc/Introduction_to_LLMs/blob/main/Amalin_Cohen_Assignment_01_Introduction__to__LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Replace 'your_key_here' with your actual API key\n",
        "# env_content = \"\"\"\n",
        "# OPENAI_API_KEY=key\n",
        "# GEMINI_API_KEY=key\n",
        "# \"\"\"\n",
        "\n",
        "# with open(\".env\", \"w\") as f:\n",
        "#     f.write(env_content.strip())\n",
        "\n",
        "# print(\".env file created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rqIqqbA9s1v",
        "outputId": "a75df10f-3ee0-419a-b78c-e0cd4d4b04c1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".env file created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24MQeFuQjY-t"
      },
      "source": [
        "# Introduction to Large Language Models and Prompt Engineering\n",
        "\n",
        "**Course:** GenAI development and LLM applications\n",
        "\n",
        "\n",
        "**Instructors:** Ori Shapira, Yuval Belfer\n",
        "\n",
        "**Semester:** Summer\n",
        "    \n",
        "## Overview\n",
        "\n",
        "This assignment provides a **hands‑on tour of the GenAI workflow**: from calling different kinds of language models to critically comparing their outputs and evaluating real‑world products built on top of them. You will practice with both *open‑source checkpoints* (via Hugging Face) and *hosted APIs* from OpenAI and Google, and reflect on where each approach shines.\n",
        "\n",
        "Along the way you will explore how model size, instruction‑tuning, and deployment UX influence quality, cost, and user trust. The notebook culminates in a creative exercise where you invent a GenAI product to fill an unmet need—connecting technical capability to business value.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- **Load and run** open‑source, OpenAI, and Google Gemini models using their latest SDKs/APIs.\n",
        "- **Compare** small (≤ 3B) and large (≥ 7 B) models on simple vs. complex prompts.\n",
        "- **Assess** output quality across dimensions such as correctness, coherence, and creativity.\n",
        "- **Evaluate** end‑user GenAI products, identifying UX trade‑offs and guardrails.\n",
        "- **Design** a new GenAI product concept aligned with a specific business pain‑point.\n",
        "\n",
        "## Prerequisites\n",
        "- Basic Python knowledge\n",
        "- Familiarity with Jupyter notebooks\n",
        "- Internet connection for API calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5FE7dXVjY-v"
      },
      "source": [
        "## \\*\\*IMPORTANT NOTE\n",
        "\n",
        "* **DO NOT HESITATE TO USE Gen AI tools such as ChatGPT to HELP you solve this**\n",
        "\n",
        "* **DO NOT USE Gen AI tools such as ChatGPT to solve this for you**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSDdE5yWjY-v"
      },
      "source": [
        "# 1. Main stream APIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKOD6W1CjY-v"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ist6EdU-jY-w"
      },
      "source": [
        "### Setup Guide: OpenAI & Google Gemini APIs\n",
        "\n",
        "Follow these steps to get your notebook ready to call the mainstream LLM APIs used in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qXX_uoJjY-w"
      },
      "source": [
        "#### 1. Install Python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8q_BoUBCjY-w",
        "outputId": "2f4a0605-cbaa-4cd3-f928-2e9fb3f24fe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.95.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.175.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary Python packages\n",
        "%pip install --upgrade openai google-generativeai python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUTZUKGDjY-x"
      },
      "source": [
        "If you prefer installing from a terminal instead of within Colab/Jupyter, the equivalent command is:\n",
        "\n",
        "```bash\n",
        "pip install --upgrade openai google-generativeai python-dotenv\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek8O5E5YjY-x"
      },
      "source": [
        "#### 2. Obtain your API keys\n",
        "\n",
        "| Provider | Where to generate the key |\n",
        "|----------|--------------------------|\n",
        "| **OpenAI** | 1. Sign in at <https://platform.openai.com/>  <br>2. Go to **API Keys → Create new secret key**  <br>3. Copy the key (`sk-…`) |\n",
        "| **Google Gemini** | 1. Visit <https://aistudio.google.com/app/apikey>  <br>2. Click **Create API key** and follow the prompts  <br>3. Copy the key (`AI-…`) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX7eUseVjY-x"
      },
      "source": [
        "#### 3. Store keys as environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl-EuocOjY-x"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # Replace the placeholders below with your real keys.\n",
        "# os.environ['OPENAI_API_KEY'] = \"sk-...\"\n",
        "# os.environ['GEMINI_API_KEY'] = \"AI-...\"\n",
        "\n",
        "# print(\"Environment variables set for this notebook session ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olBnJvvbjY-x"
      },
      "source": [
        "Alternatively, you can create a `.env` file in your project directory with the following contents:\n",
        "\n",
        "```text\n",
        "OPENAI_API_KEY=sk-...\n",
        "GEMINI_API_KEY=AI-...\n",
        "```\n",
        "\n",
        "Then load the variables automatically in Python using **python-dotenv**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BX5infNqjY-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd14f01-be0c-4b08-8425-a7d5e5f917b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # Loads variables from .env into the current environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MtgbOt3jY-y"
      },
      "source": [
        "#### 4. Quick verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6giGX-mnjY-y",
        "outputId": "8e31c2b9-0596-4460-c3c9-866e2dc1f508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI models: ['gpt-4-0613', 'gpt-4', 'gpt-3.5-turbo']\n",
            "\n",
            "Available Gemini models that support generateContent:\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "import google.generativeai as genai\n",
        "# from google.colab import userdata\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "# genai.configure(api_key= userdata.get('GEMINI_API_KEY') )\n",
        "\n",
        "# Try listing available OpenAI models (show first 3)\n",
        "print(\"OpenAI models:\", [m.id for m in openai.models.list().data[:3]])\n",
        "\n",
        "# List available Gemini models that support generateContent\n",
        "print(\"\\nAvailable Gemini models that support generateContent:\")\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7jetwzTjY-y"
      },
      "source": [
        "## Using OAI API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TDGubA1CjY-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b8243c-b4bc-417b-992e-8a75f689c427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears pink under certain conditions due to the scattering of sunlight in the atmosphere. This phenomenon is largely influenced by the scattering process known as Rayleigh scattering. Here's a simplified explanation:\n",
            "\n",
            "1. **Sunlight Composition**: Sunlight is composed of a spectrum of colors, each with different wavelengths. Blue light has shorter wavelengths, while red light has longer wavelengths.\n",
            "\n",
            "2. **Rayleigh Scattering**: During the day, shorter wavelengths (blue and violet light) are scattered in all directions by the gases and particles in the Earth's atmosphere, which is why the sky appears blue.\n",
            "\n",
            "3. **Sunrise and Sunset**: During sunrise and sunset, the sun is positioned lower on the horizon, meaning its light has to pass through a thicker part of the atmosphere. This extended path causes more scattering of the shorter wavelengths (like blue and green), leaving the longer wavelengths (reds, oranges, and pinks) to dominate the sky's appearance.\n",
            "\n",
            "4. **Particulate Matter and Pollution**: The presence of particles, dust, or pollution can enhance the scattering of red and orange wavelengths, sometimes resulting in a pink sky.\n",
            "\n",
            "5. **Weather Conditions**: Certain cloud formations and atmospheric conditions can also reflect and refract light in such a way that enhances the pink hues, especially when the light interacts with different particles or moisture in the air.\n",
            "\n",
            "As a result of these factors, under certain conditions, especially around sunrise or sunset and when atmospheric conditions are just right, the sky can appear pink.\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Implement `call_openai`\n",
        "# Your task is to complete the function below so that it sends a prompt to the OpenAI\n",
        "# Chat Completion API and returns the assistant's reply as a **string**.\n",
        "#\n",
        "# Requirements ✍️\n",
        "# ------------\n",
        "# 1. Read your key from the environment variable `OPENAI_API_KEY` (do **not** hard‑code keys).\n",
        "# 2. Use `openai.ChatCompletion.create` with the given *model* (default: `gpt-4o-mini`).\n",
        "# 3. The function should take a single user *prompt* and return the model's reply **text only**.\n",
        "# 4. Raise a clear `EnvironmentError` if the key is missing.\n",
        "# 5. Let any `openai.error.OpenAIError` exceptions propagate––don’t swallow them.\n",
        "#\n",
        "# Example\n",
        "# -------\n",
        "# >>> call_openai(\"Explain why the sky is blue.\")\n",
        "# 'Rayleigh scattering causes shorter (blue) wavelengths ...'\n",
        "#\n",
        "# This exercise tests your ability to work with third‑party GenAI REST APIs in Python.\n",
        "#\n",
        "# 🚨 Delete the `raise NotImplementedError` line once you add your code.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "def call_openai(prompt: str, model: str = \"gpt-4o\") -> str:\n",
        "    \"\"\"Send *prompt* to the OpenAI Chat Completion API and return the reply text.\"\"\"\n",
        "\n",
        "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise EnvironmentError(\"Missing OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    # Let OpenAIError propagate naturally if it occurs\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"Explain why the sky is blue.\"\n",
        "print(call_openai(prompt))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gDUShHnjY-y"
      },
      "source": [
        "## Using Google Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwuSXyycjY-y"
      },
      "outputs": [],
      "source": [
        "# Exercise: Implement `call_gemini`\n",
        "# Your task is to complete the function below so that it sends a prompt to Google’s **Gemini** model\n",
        "# (via the `google-generativeai` Python SDK) and returns the model's reply as a **string**.\n",
        "#\n",
        "# Requirements ✍️\n",
        "# ------------\n",
        "# 1. Read your key from the environment variable `GEMINI_API_KEY`.\n",
        "# 2. Use `google.generativeai.GenerativeModel(model).generate_content` to get a response.\n",
        "# 3. Accept the same arguments as `call_openai` (prompt, model) and return *text only*.\n",
        "# 4. Raise `EnvironmentError` if the key is missing.\n",
        "#\n",
        "# Example\n",
        "# -------\n",
        "# >>> call_gemini(\"Suggest three sci‑fi book titles set on Mars.\")\n",
        "# '1. Red Horizon ...'\n",
        "#\n",
        "# 🚨 Delete the `raise NotImplementedError` once you add your code.\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "def call_gemini(prompt: str, model: str = \"gemini-pro\") -> str:\n",
        "    \"\"\"Send *prompt* to the Google Gemini API and return the reply text.\"\"\"\n",
        "    # TODO: Implement this function following the requirements above.\n",
        "    raise NotImplementedError(\"call_gemini() is not implemented yet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240ehahpjY-y"
      },
      "source": [
        "# Open source (Hugging Face)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aunvloFmjY-y"
      },
      "source": [
        "\n",
        "\n",
        "1. Use **Gemma 3 4B** model.\n",
        "2. Ensure your Hugging Face access token (`HF_TOKEN`) is set correctly in your environment (e.g., `export HF_TOKEN=YOUR_TOKEN`), or use `huggingface-cli login`.\n",
        "3. Confirm that the student has **accepted the model terms** for Gemma on Hugging Face before attempting to download or load the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSf40_sqjY-y"
      },
      "source": [
        "## Setup\n",
        "\n",
        "In this subsection we'll install and configure the dependencies needed to load an **open‑source large language model (LLM)** from **Hugging Face**.\n",
        "\n",
        "1. We install the latest versions of the `transformers`, `accelerate`, and `bitsandbytes` libraries.\n",
        "2. If the model you want to use is behind a gated license (e.g. *Llama 2*), create a free Hugging Face account, accept the model license, and export your **HF token** as an environment variable so that `transformers` can download it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUfhUGKQjY-z"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries (run once)\n",
        "!pip -q install --upgrade \"transformers[torch]\" accelerate bitsandbytes --progress-bar off\n",
        "\n",
        "# Set your Hugging Face access token\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPWm1IMTjY-z"
      },
      "source": [
        "## Running Gemma‑3‑4B via `transformers` pipeline\n",
        "\n",
        "Below we load the **Gemma‑3‑4B** instruction‑tuned checkpoint (`google/gemma-3-4b-it`) and generate a short response.  \n",
        "Make sure you have accepted the model license on Hugging Face **and** that your `HF_TOKEN` is set (or run `huggingface-cli login`) before running the cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppROWgqPjY-z"
      },
      "source": [
        "**IMPORTANT** If you are using google colab change your notebook's runtime type to include a GPU. Go to \"Runtime\" -> \"Change runtime type\" and select \"T4 GPU\" under \"Hardware accelerator\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-quWJ-WjY-z"
      },
      "source": [
        "**NOTE:** you can use ***FlashAttention2*** for faster infference. you'll have to install `!pip install flash_attn --no-build-isolation`, update `attn_implementation` and hope that's it :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH-KWjQxjY-z"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    Gemma3ForConditionalGeneration,\n",
        "    pipeline,\n",
        ")\n",
        "import torch\n",
        "\n",
        "# Instruction‑tuned Gemma 3 checkpoint\n",
        "model_id = \"google/gemma-3-4b-it\"\n",
        "\n",
        "# Check for CUDA availability and set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Determine torch_dtype based on device and availability\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16 if torch.cuda.is_available() else \"auto\"\n",
        "print(f\"Using torch_dtype: {torch_dtype}\")\n",
        "\n",
        "\n",
        "# Load tokenizer & model (first run ~8‑10 GB download)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    attn_implementation=\"eager\",             # Fallback to eager attention / You can try \"flash_attention_2\" if you have a GPU with flash attention support\n",
        "    device_map=device,                       # Explicitly set device\n",
        "    torch_dtype=torch_dtype,                 # use bf16/fp16 if available\n",
        "    token=True                               # pick up HF_TOKEN env var\n",
        ")\n",
        "\n",
        "# Build a text‑generation pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "prompt = \"Explain the difference between supervised and unsupervised learning in two sentences.\"\n",
        "response = generator(prompt)[0][\"generated_text\"]\n",
        "print(\"\\n--- Generated Response ---\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z4X5ZEcjY-z"
      },
      "source": [
        "## Comparing Output Quality: Gemma 3 4B vs. Gemini / GPT‑4o\n",
        "\n",
        "In this section you will **run the *same prompt*** through three different models and compare their outputs:\n",
        "\n",
        "1. **Gemma 3 4B** – use the `generator` pipeline you built earlier (`google/gemma-3-4b-it` checkpoint).  \n",
        "2. **Gemini 1.5 Pro (call_gemini)** – use your `call_gemini(prompt)` helper.  \n",
        "3. **GPT‑4o (call_openai)** – use your `call_openai(prompt)` helper.\n",
        "\n",
        "### What to do\n",
        "* Pick a single prompt of your choice (keep it identical for all three calls).  \n",
        "* Capture each model’s response.  \n",
        "* In a short paragraph, compare the outputs along these axes: **factual accuracy / hallucinations, completeness, style / tone, and overall usefulness**.  \n",
        "* Optionally repeat with a *second* prompt that stresses reasoning or creativity to see whether conclusions change.\n",
        "\n",
        "**Fill in the code cell below and then write your reflection in the provided markdown cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQD92dFHjY-z"
      },
      "outputs": [],
      "source": [
        "# ▶️ Compare outputs from Gemma 3‑4B, Gemini, and GPT‑4o\n",
        "prompt = \"Explain the most important differences between classical mechanics and quantum mechanics in two paragraphs.\"\n",
        "\n",
        "# Gemma (pipeline defined earlier)\n",
        "gemma_out = generator(prompt)[0][\"generated_text\"]\n",
        "\n",
        "# Gemini (helper function you implemented)\n",
        "gemini_out = call_gemini(prompt)\n",
        "\n",
        "# GPT‑4o (helper function you implemented)\n",
        "gpt4o_out = call_openai(prompt)\n",
        "\n",
        "print(\"\\n--- Gemma 3‑4B ---\\n\", gemma_out)\n",
        "print(\"\\n--- Gemini ---\\n\", gemini_out)\n",
        "print(\"\\n--- GPT‑4o ---\\n\", gpt4o_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwwUMkbfjY-z"
      },
      "source": [
        "# Try out a GenAI‑Based Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKwT6QqEjY-0"
      },
      "source": [
        "\n",
        "\n",
        "Choose **one** generative‑AI product (for example:  \n",
        "* **Cursor** – an AI‑powered code editor,  \n",
        "* **GitHub Copilot** – an AI pair‑programmer,  \n",
        "* **Perplexity AI** – an answer‑engine / research assistant,  \n",
        "* **Claude**, **Notion AI**, **Llama Index Lobe**,\n",
        "or any other tool that interests you).  \n",
        "\n",
        "Spend **10–15 minutes** exploring its features. Then:\n",
        "\n",
        "1. Briefly describe the *task or workflow* you tried.  \n",
        "2. List **two things you liked** and **two things you wish were better**.  \n",
        "3. Reflect on how underlying model capabilities (e.g., context window, fine‑tuning, retrieval, chain‑of‑thought) manifest in the product experience.  \n",
        "4. (Optional) Include a screenshot or snippet illustrating something interesting you observed.\n",
        "\n",
        "Use the markdown cell below to record your reflections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj43JWzOjY-0"
      },
      "source": [
        "# What GenAI products does not exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eXTwys6jY-0"
      },
      "source": [
        "## Thinking Exercise – Design a New GenAI Product\n",
        "\n",
        "Invent **one GenAI‑powered product that does not yet exist** but could solve a real problem. Write **100‑150 words** addressing the prompts below.\n",
        "\n",
        "**Prompts**\n",
        "1. **Product name & one‑line pitch**  \n",
        "2. **Target users / market segment**  \n",
        "3. **Pain point / business need** – why current solutions are insufficient  \n",
        "4. **User flow & where GenAI fits** – describe the key interactions  \n",
        "5. **Technical or ethical challenges** – data, safety, IP, bias, etc.  \n",
        "6. **MVP success metric** – what would you measure to confirm traction  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRyDfYtkjY-0"
      },
      "source": [
        "### ✍️ Your concept\n",
        "\n",
        "**1. Product name & pitch**:  \n",
        "\n",
        "**2. Target users**:  \n",
        "\n",
        "**3. Pain point**:  \n",
        "\n",
        "**4. User flow**:  \n",
        "\n",
        "**5. Challenges**:  \n",
        "\n",
        "**6. Success metric**:  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}